{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi, log\n",
    "from functools import wraps\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def cache_fn(f):\n",
    "    cache = None\n",
    "    @wraps(f)\n",
    "    def cached_fn(*args, _cache = True, **kwargs):\n",
    "        if not _cache:\n",
    "            return f(*args, **kwargs)\n",
    "        nonlocal cache\n",
    "        if cache is not None:\n",
    "            return cache\n",
    "        cache = f(*args, **kwargs)\n",
    "        return cache\n",
    "    return cached_fn\n",
    "\n",
    "# structured dropout, more effective than traditional attention dropouts\n",
    "\n",
    "def dropout_seq(seq, mask, dropout):\n",
    "    b, n, *_, device = *seq.shape, seq.device\n",
    "    logits = torch.randn(b, n, device = device)\n",
    "\n",
    "    if exists(mask):\n",
    "        logits = logits.masked_fill(~mask, -torch.finfo(logits.dtype).max)\n",
    "\n",
    "    keep_prob = 1. - dropout\n",
    "    num_keep = max(1,  int(keep_prob * n))\n",
    "    keep_indices = logits.topk(num_keep, dim = 1).indices\n",
    "\n",
    "    batch_indices = torch.arange(b, device = device)\n",
    "    batch_indices = rearrange(batch_indices, 'b -> b 1')\n",
    "\n",
    "    seq = seq[batch_indices, keep_indices]\n",
    "\n",
    "    if exists(mask):\n",
    "        seq_counts = mask.sum(dim = -1)\n",
    "        seq_keep_counts = torch.ceil(seq_counts * keep_prob).int()\n",
    "        keep_mask = torch.arange(num_keep, device = device) < rearrange(seq_keep_counts, 'b -> b 1')\n",
    "\n",
    "        mask = mask[batch_indices, keep_indices] & keep_mask\n",
    "\n",
    "    return seq, mask\n",
    "\n",
    "# helper classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, context_dim = None):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context = normed_context)\n",
    "\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim = None, heads = 8, dim_head = 64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
    "\n",
    "    def forward(self, x, context = None, mask = None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h = h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        # attention, what we cannot get enough of\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# main class\n",
    "\n",
    "class PerceiverEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        depth,\n",
    "        dim,\n",
    "        queries_dim,\n",
    "        logits_dim = None,\n",
    "        num_latents = 512,\n",
    "        latent_dim = 512,\n",
    "        cross_heads = 1,\n",
    "        latent_heads = 8,\n",
    "        cross_dim_head = 64,\n",
    "        latent_dim_head = 64,\n",
    "        weight_tie_layers = False,\n",
    "        decoder_ff = False,\n",
    "        seq_dropout_prob = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seq_dropout_prob = seq_dropout_prob\n",
    "\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
    "\n",
    "        self.cross_attend_blocks = nn.ModuleList([\n",
    "            PreNorm(latent_dim, Attention(latent_dim, dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = dim),\n",
    "            PreNorm(latent_dim, FeedForward(latent_dim))\n",
    "        ])\n",
    "\n",
    "        get_latent_attn = lambda: PreNorm(latent_dim, Attention(latent_dim, heads = latent_heads, dim_head = latent_dim_head))\n",
    "        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))\n",
    "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        cache_args = {'_cache': weight_tie_layers}\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                get_latent_attn(**cache_args),\n",
    "                get_latent_ff(**cache_args)\n",
    "            ]))\n",
    "\n",
    "        self.decoder_cross_attn = PreNorm(queries_dim, Attention(queries_dim, latent_dim, heads = cross_heads, dim_head = cross_dim_head), context_dim = latent_dim)\n",
    "        self.decoder_ff = PreNorm(queries_dim, FeedForward(queries_dim)) if decoder_ff else None\n",
    "\n",
    "        self.to_logits = nn.Linear(queries_dim, logits_dim) if exists(logits_dim) else nn.Identity()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data,\n",
    "        mask = None,\n",
    "        queries = None\n",
    "    ):\n",
    "        b, *_, device = *data.shape, data.device\n",
    "\n",
    "        x = repeat(self.latents, 'n d -> b n d', b = b)\n",
    "\n",
    "        cross_attn, cross_ff = self.cross_attend_blocks\n",
    "\n",
    "        # structured dropout (as done in perceiver AR https://arxiv.org/abs/2202.07765)\n",
    "\n",
    "        if self.training and self.seq_dropout_prob > 0.:\n",
    "            data, mask = dropout_seq(data, mask, self.seq_dropout_prob)\n",
    "\n",
    "        # cross attention only happens once for Perceiver IO\n",
    "\n",
    "        x = cross_attn(x, context = data, mask = mask) + x\n",
    "        x = cross_ff(x) + x\n",
    "\n",
    "        # layers\n",
    "\n",
    "        for self_attn, self_ff in self.layers:\n",
    "            x = self_attn(x) + x\n",
    "            x = self_ff(x) + x\n",
    "\n",
    "        if not exists(queries):\n",
    "            return x\n",
    "\n",
    "        # make sure queries contains batch dimension\n",
    "\n",
    "        if queries.ndim == 2:\n",
    "            queries = repeat(queries, 'n d -> b n d', b = b)\n",
    "\n",
    "        # cross attend from decoder queries to latents\n",
    "        \n",
    "        latents = self.decoder_cross_attn(queries, context = x)\n",
    "\n",
    "        # optional decoder feedforward\n",
    "\n",
    "        if exists(self.decoder_ff):\n",
    "            latents = latents + self.decoder_ff(latents)\n",
    "\n",
    "        # final linear out\n",
    "\n",
    "        return self.to_logits(latents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 151]),\n",
       " torch.Size([4, 151]),\n",
       " torch.Size([4, 151, 151]),\n",
       " torch.Size([4, 151, 151]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = torch.randint(0, 3000, (4, 151))\n",
    "adj_matrix = torch.randint(0, 1000, (4, 151, 151))\n",
    "pad_id = 0\n",
    "\n",
    "masked_nodes, node_masks, masked_adj, adj_masks = mask_nodes_and_adj(nodes, adj_matrix, pad_id)\n",
    "masked_nodes.shape, node_masks.shape, masked_adj.shape, adj_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg-augmented-lm-3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
