{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcomoldovan/.pyenv/versions/3.11.5/envs/ipykernel-3.11.5/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "from torchmetrics.text import BLEUScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "bleu = BLEUScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.4548991918563843\n"
     ]
    }
   ],
   "source": [
    "pred = torch.randint(0, 32000, (64, 128)).cpu().numpy().astype(str)\n",
    "pred = [\" \".join(p) for p in pred]\n",
    "trg = torch.randint(0, 32000, (64, 128)).cpu().numpy().astype(str)\n",
    "trg = [\" \".join(t) for t in trg]\n",
    "# write a few sentences\n",
    "pred = [\"There is a cat on the mat and a dog on the cat\", \"A dog is on the mat and a cat is on the dog\", \"The chicken is on the mat and a dog is on the cat\"]\n",
    "trg = [[\"A dog is on the mat and a cat is on the dog\"], [\"There is a cat on the mat and a dog on the cat\"], [\"A dog is on the mat and a cat is on the dog\"], [\"The chicken is on the mat and a dog is on the cat\"]]\n",
    "bleu_score = bleu(pred, trg)\n",
    "print(f\"BLEU score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7598)\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.text import BLEUScore\n",
    "preds = ['the cat is on the mat']\n",
    "target = [['there is a cat on the mat', 'a cat is on the mat']]\n",
    "bleu = BLEUScore()\n",
    "score = bleu(preds, target)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2147)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = (bleu_score, score)\n",
    "sum(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['101 1247 1110 170 5855 1113 1103 22591 1105 170 3676 1113 1103 5855 102', '101 138 3676 1110 1113 1103 22591 1105 170 5855 1110 1113 1103 3676 102', '101 1109 9323 1110 1113 1103 22591 1105 170 3676 1110 1113 1103 5855 102']\n",
      "BLEU score text: 0.4548991918563843\n",
      "BLEU score ids: 0.40449997782707214\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "pred = [\"There is a cat on the mat and a dog on the cat\", \"A dog is on the mat and a cat is on the dog\", \"The chicken is on the mat and a dog is on the cat\"]\n",
    "trg = [\"A dog is on the mat and a cat is on the dog\", \"There is a cat on the mat and a dog on the cat\", \"A dog is on the mat and a cat is on the dog\", \"The chicken is on the mat and a dog is on the cat\"]\n",
    "trg_for_bleu = [[t] for t in trg]\n",
    "\n",
    "pred_ids = tokenizer(pred, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "pred_ids = [\" \".join(p.cpu().numpy().astype(str)) for p in pred_ids[\"input_ids\"]]\n",
    "print(pred_ids)\n",
    "\n",
    "trg_ids = tokenizer(trg, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "trg_ids = [[\" \".join(t.cpu().numpy().astype(str))] for t in trg_ids[\"input_ids\"]]\n",
    "\n",
    "bleu = BLEUScore()\n",
    "\n",
    "score_text = bleu(pred, trg_for_bleu)\n",
    "score_ids = bleu(pred_ids, trg_ids)\n",
    "print(f\"BLEU score text: {score_text}\")\n",
    "print(f\"BLEU score ids: {score_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel-3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
