_target_: src.models.lit_module.KGAugmentedLMLitModule

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10

net:
  _target_: src.models.components.kg_augmented_lm.KGAugmentedLM
  latent_dim: 32
  num_unimodal_encoder_layers: 1
  num_unimodal_encoder_heads: 1
  fusion_model: 'bottleneck'
  bottleneck_width: 4
  num_fusion_layers: 1
  num_fusion_heads: 1
  text_vocab_size: 50262 # gpt2 with added special tokens
  graph_vocab_size: 31092 # wikigraphs: 31092, wikidata5m: 4594485
  num_edge_types: 31092 # wikigraphs: 522, wikidata5m: 822 #! have to set to graph_vocab_size for wikigraphs because nodes and edges are saved in the same vocab
  modality_encoding: 'sinusoidal'